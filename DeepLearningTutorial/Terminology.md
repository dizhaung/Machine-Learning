# 机器学习中的杂谈

## Batch Size
- 合理范围内增大Batch Size的好处
    - 内存利用率提高了, 大矩阵乘法的并行化效率提高
    - 跑完一次epoch(全数据集)所需要的迭代次数减小,对于相同数据量的处理速度进一步加快
    - 在一定范围内,一般来说batch size越大,其确定的下降方向越准,引起的训练震荡越小
    
- 盲目增大Batch Size的坏处
    - 内存利用率提高了,但是内存容量可能撑不住了
    - 跑完一次epoch（全数据集）所需要的迭代次数减少,但是想要达到相同的精度,其所花费的时间大大增加了,
    从而对参数的修正也就显得更加缓慢
    - batch size 大到一定的程度,其确定的下降方向已经基本不再变化

## Fine Tuning
- 流程
    - 准备数据集(包括训练数据和测试数据)
    - 均一化数据
    - 修改网络最后一层的输出类别数, 以及最后一层网络的名称, 加大最后一层的参数学习速率
    - 调整solver的配置参数
    - 加载预训练模型的参数, 启动训练
    
- 参数调整
    - 一般fine tuning的方式, 都是把learning rate(solver.prototxt)调低(为原来的十分之一), 之后把训练模型的最后一层
    或者两层的学习速率调大一点 ------- 这相当于, 把模型的前面那些层的学习调低, 使得参数更新的慢一点以达到微调的目的. 
    但有可能会导致整个收敛速度变慢, 因此还需要增加最大迭代次数.
    
    - Total Loss 不收敛的情况可以尝试以下几种方法:
        - 调小solver里的初始学习率
        - 数据量小的话, 可以只对最后几层的权重进行调整, 前面权重保持不变, 即把前面的学习率置为0, 
        一定程度上限制了激活的大小, 这样就限制了某一过大的误差的影响, 这样可以避免迭代方向出现过大的变化.
        - 在GPU可承受的范围内调大batch_size的大小(贾扬清的解释: 理论上batch小是不会影响收敛的. 
        小batch主要的问题是在FC层的计算可能会不是很efficient,如果实在无计可施,可以试试; 网上其他博主的解释: 
        batch比较小 ,导致样本覆盖面过低,产生了非常多的局部极小点,在步长和方向的共同作用下,导致数据产生了震荡,导致了不收敛)