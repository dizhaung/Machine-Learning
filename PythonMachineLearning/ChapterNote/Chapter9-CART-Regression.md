# CART树回归

## 前言

CART树回归属于一种局部的回归算法, 通过将全局的数据集划分成多分容易建模的数据集, 
这样在每一个局部的数据集上进行局部的回归建模.

## CART

- 在CART分类树中, 利用Gini指数作为划分树的指标, 但对于连续数据, Gini指数则不适合(Gini指数表示的是数据的混乱程度)
- 在CART回归树中, 利用样本与平均值的差的平方和(方差思想)作为划分回归树的指标, 
公式为: ![](MularGif/Chapter9Gif/CART%20Regression.gif)
- 利用残差平方和评估决策树的好坏

### CART分类树构建步骤

- 对于当前训练数据集, 遍历所有属性及其所有可能的切分点, 寻找最佳切分属性及其最佳切分点, 
使得切分之后的基尼指数最小, 利用该最佳属性及其最佳切分点将训练数据集切分成两个子集, 分别对应左右子树.
- 对上一步生成的左右数据子集递归调用第一步, 直至满足停止条件.
- 生成CART决策树

### CART回归树剪枝

- 为什么剪枝?
    - 当树中的节点对样本一直划分下去时, 会出现最极端的情况(每个叶子节点仅包含一个样本, 值即为该样本的标签的值),
     出现"过拟合"问题, 对新样本的预测效果较差, 所以需要对回归树进行剪枝
     
- 剪枝方法
    - 前剪枝
        - 指在生成CART回归树的过程中对树的深度进行控制, 防止生成过多的叶子节点.
    - 后剪枝
        - 将训练样本数据分成两个部分, 一部分用来训练CART树模型(训练数据), 
        另外一部分对生成的CART数模型进行剪枝(验证数据). 