# Python 机器学习算法总结

## Logistic Regression

属于线性的分类模型, 主要处理二分类问题, 使用阈值函数sigmoid(带有均一化功能), 求导转换为凸优化或者非凸优化问题. 
由于导函数的形式为f(x)[1-f(x)], 因此引入伯努利分布的似然函数(Log似然函数求解似然函数的最大值问题). 
并通过梯度下降的方法解出凸优化问题的解(步长影响收敛速度及震荡问题).

逻辑回归可以看作是一个单层的感知器(没有隐层)结构, 参考BP.

## Softmax Regression

是Logistic Regression在多分类问题上的扩展, 都是利用假设函数估计样本所属类别的概率, 利用损失函数及梯度下降求得最优解.

## Factorization Machine

- 主要针对特征之间不独立的情况
- 基于矩阵分解的思想, 用来处理非线性可分的问题.一般用来处理回归、二分类、排序问题.由于数据稀疏性的问题, 引入基于
矩阵分解思想的辅助向量对模型求解. 对于高度稀疏的数据场景具有较好的效果.

## Support Vector Machine

- 适用于二分类问题
- SVM它本质上即是一个分类方法,用w^T+b定义分类函数,于是求w、b,为寻最大间隔,引出1/2||w||^2,继而引入拉格朗日因子,
化为对拉格朗日乘子a的求解（求解过程中会涉及到一系列最优化或凸二次规划等问题）,如此,求w.b与求a等价,
而a的求解可以用一种快速学习算法SMO,至于核函数,是为处理非线性情况,若直接映射到高维计算恐维度爆炸,
故在低维计算,等效高维表现.
- 拉格朗日乘子法是求解条件极值的解析方法, 可将所有约束的优化模型问题转化为无约束极值问题的求解.
而KKT条件属于拉格朗日乘子法的泛化.
- <font color=#9F65F9>SMO真令人头大</font>, [来再头大一次](https://www.cnblogs.com/pinard/p/6111471.html)

## Random Forest

Random Forest属于集成学习(选择训练多个分类模型, 并将各自的预测结果组合起来得到最终的预测)的一种.

主要的决策树模型有以下几种:
- ID3算法(分类树): 以信息熵和信息增益为衡量标准
- C4.5算法(分类树): 属于ID3分类树的改进, 使用信息增益率为衡量标准
- CART算法(分类回归树): 既可处理分类问题, 也可处理回归问题, 使用Gini指数为衡量标准

决策树的剪枝优化, [详情](Part1-Classification/Chapter5-Random-Forest.md)

## Back Propagation Neural Network

核心思想: 利用前向传播(通过激活函数和各个网络层一次处理)求得输出并计算误差, 再用反向传播的思想调整参数直至
返回最终的神经网络模型.

具有很强的非线性映射能力和柔性的网络结构.

较于LR算法(只能处理线性问题), 引入多层感知器解(加入隐含层)决非线性问题.

## Linear Regression

基本的线性回归算法, 优化算法[牛顿法](https://blog.csdn.net/google19890102/article/details/41087931)的引入, 
逼近速度比梯度下降速度更快且能高度逼近最优值.
- 利用一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似(运用泰勒展开), 更新w

为解决处理复杂数据而出现的欠拟合问题, 局部加权算法的提出. 局部加权线性回归每次预测都需要全部的训练数据, 进行预测, 
属于非参数学习算法

## 岭回归和Lasso回归

为了解决基本线性回归算法对特征之间存在相关性处理能力不足的情况. 在平方误差的基础上添加正则项, 提高泛化能力.

拟牛顿法的引入解决Lasso回归在w<sub>j</sub>=0处不可导而造成的基于梯度的优化算法不适用问题, 

## CART 树回归

CART树回归算法属于一种局部的回归算法, 打破了基本回归模型及岭回归和Lasso回归关于全局数据之间是线性关系的限定, 能够
处理非线性回归.

且由于局部加权线性回归属于非参数学习, 不适合大量数据的情况.

CART树回归不适用Gini指数作为划分树的指标(连续数据不适合), 采用样本与平均值的差的平方和作为划分回归树的指标.

## K-Means

聚类分析是在数据中发现数据对象之间的关系, 将数据进行分组, 组内的相似性越大, 组间的差别越大, 则聚类效果越好.

K-Means通过距离函数将数据划分成不同的簇集合, 算法实现较简单, 但在大规模数据收敛较慢.

## Mean Shift

K-Means和K-Means++都需要事先指定类别个数k, 而Mean Shift中聚类中心是通过在给定区域中的样本的均值来确定的. 通过不断更新
聚类中心, 直至不在改变为止.

Mean Shift算法,是一个迭代的步骤,即先算出当前点的偏移均值,将该点移动到此偏移均值,
然后以此为新的起始点,继续移动,直到满足最终的条件.

## DBSCAN

基于密度的聚类算法较基于距离的聚类聚类算法而言, 能够较好地处理非球状结构(能发现任意形状的聚类)的数据.


## Label Propagation

基于图的半监督学习算法, 用已标记的节点的标签信息去预测未标记节点的标签信息.

标签传播: 随着社区标签的不断传播, 连接紧密的节点将具有共同的标签. 其算法速度非常快, 且无需任何优化参数, 
社区个数由算法自己决定.

## 协同过滤算法

通过从用户的历史行为数据挖掘出不同用户、不同商品之间的关联性.基于欧氏距离、皮尔逊相关系数、余弦相似度等度量
相关性. 从而做到精准推荐, 推荐有基于用户、基于item、基于模型等推荐方法. 

基于user的方法在处理稀疏数据及扩展性(大量数据情况)不足, 基于模型是通过组合多种方法进行精准推荐,
是目前主流的推荐算法.

## 基于矩阵分解的推荐算法

属于基于模型的协同过滤算法的一种,可以实现实时推荐. 
矩阵分解是一个离线的过程, 工业界使用的很少, 但有一些近似的方法(包括深度学习方面近似的做法).
- 基于矩阵分解: 损失函数为平方损失
- 基于非负矩阵分解(分解后的矩阵非负, 因此优化问题使用乘法更新规则, 保证非负性): 损失函数可为平方损失、KL散度(相对熵)

## 基于图的推荐算法

二部图的涉及

PageRank(网页排名算法): 计算出的PR值是每个节点相对于全局的重要性程度

PersonalRank: 用于计算所有的商品及诶单对于某个用户节点U(由于每个用户推荐都需要重新计算, 因此不能实现实时推荐)
的重要性程度, 属于推荐算法的一种.


## 杂谈

- [模型选择](MularGif/modul·e%20choice.jpg)